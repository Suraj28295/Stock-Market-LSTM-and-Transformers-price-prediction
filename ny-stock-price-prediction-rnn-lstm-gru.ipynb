{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Libraries and settings <a class=\"anchor\" id=\"1-bullet\"></a> ","metadata":{"_uuid":"8ec772a02eb88c3446fe0f689b2e1ee8ec9deb6b","_cell_guid":"1426ca5e-ed0c-4766-ac74-cd1dd38c680c"}},{"cell_type":"code","source":"# !pip install numpy==1.13.3 Keras==2.0.5 wrapt~=1.12.1 lxml==4.0.0 \n# !pip install --upgrade pandas colorama \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade tensorflow\nimport tensorflow as tf\nprint('Tensorflow version: {}'.format(tf.__version__))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport sklearn\nimport sklearn.preprocessing\nimport datetime\nimport os\nimport matplotlib.pyplot as plt\n\n\npd.set_option('display.float_format', lambda x: '%.5f' % x)\nnp.set_printoptions(suppress=True)\n# split data in 80%/10%/10% train/validation/test sets\nvalid_set_size_percentage = 10 \ntest_set_size_percentage = 10 \n\n#display parent directory and working directory\nprint(os.path.dirname(os.getcwd())+':', os.listdir(os.path.dirname(os.getcwd())));\nprint(os.getcwd()+':', os.listdir(os.getcwd()));\n","metadata":{"_uuid":"78e6c0015907947ff5fade7ad76317633d27177a","_cell_guid":"5a142d3f-96b5-4378-bb01-903367721e08","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Analyze data <a class=\"anchor\" id=\"2-bullet\"></a> \n- load stock prices from prices-split-adjusted.csv\n- analyze data","metadata":{"_uuid":"9422490f417bc1fcac8e6a0a392faa7451173dac","_cell_guid":"5416ce8a-24ff-4903-a09b-b2941eae71d9"}},{"cell_type":"code","source":"# import all stock prices \ndf = pd.read_csv(\"../input/prices-split-adjusted.csv\", index_col = 0)\ndf.info()\ndf.head()\n\n# number of different stocks\nprint('\\nnumber of different stocks: ', len(list(set(df.symbol))))\nprint(list(set(df.symbol))[:10])","metadata":{"_uuid":"aef1074dd4f86a9fe5a380c83994123ce56cce9d","_cell_guid":"60dbc696-dda8-476e-8227-010a79df3aa2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"_uuid":"c06e709a1bfecb963213943dc7ce8cd559780d9b","_cell_guid":"962ea6e1-51ff-4bba-ad30-77ca5c765d59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"_uuid":"60ee56dd22397b9c545ea7dcc318f5c733d419ba","_cell_guid":"d1fc9207-adcd-44f4-85a1-065d34fd82f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"_uuid":"ef59e62b988d2b0bfd00f834a7ecc17cb927dfc9","_cell_guid":"8e1e0b13-c1da-4fe1-b971-bb2ebcad453b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\nplt.plot(df[df.symbol == 'EQIX'].open.values, color='red', label='open')\nplt.plot(df[df.symbol == 'EQIX'].close.values, color='green', label='close')\nplt.plot(df[df.symbol == 'EQIX'].low.values, color='blue', label='low')\nplt.plot(df[df.symbol == 'EQIX'].high.values, color='black', label='high')\nplt.title('stock price')\nplt.xlabel('time [days]')\nplt.ylabel('price')\nplt.legend(loc='best')\n#plt.show()\n\nplt.subplot(1,2,2);\nplt.plot(df[df.symbol == 'EQIX'].volume.values, color='black', label='volume')\nplt.title('stock volume')\nplt.xlabel('time [days]')\nplt.ylabel('volume')\nplt.legend(loc='best');","metadata":{"_uuid":"1235c99db35b562f09d6fb5721368bcab68e307d","_cell_guid":"ce84999a-306f-4586-b8d6-dc3a0ab9ab67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Subset data <a class=\"anchor\" id=\"3-bullet\"></a>","metadata":{}},{"cell_type":"code","source":"UL=45\nLL=44","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_subset=df[df.symbol.isin(df.groupby('symbol').mean()[(df.groupby('symbol').mean()['open']<UL) & (df.groupby('symbol').mean()['open']>LL)].index)]\nmin_max_scaler = sklearn.preprocessing.MinMaxScaler()\ndf_subset.symbol.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Manipulate data <a class=\"anchor\" id=\"3-bullet\"></a> \n- choose a specific stock\n- drop feature: volume\n- normalize stock data\n- create train, validation and test data sets","metadata":{"_uuid":"777e047511205f929abf0867c7c8a2edee2531ca","_cell_guid":"73ad0def-f162-42b9-a1d6-aac45be27e81"}},{"cell_type":"code","source":"# function for min-max normalization of stock\ndef normalize_data(df,col_minmax):\n    df=pd.concat([pd.DataFrame(min_max_scaler.fit_transform(df[col_minmax]),columns=col_minmax),\n                  pd.get_dummies(df['symbol'],prefix=\"symbol\").reset_index()],axis=1)\n    df.drop(['date'],1,inplace=True)\n    return df\n\n# normalize stock\ndf_stock_norm = df_subset.copy()\ndf_stock_norm = normalize_data(df_stock_norm,['open', 'close', 'low', 'high', 'volume'])\nprint('df_stock.columns.values = ', list(df_stock_norm.columns.values))\n# # create train, test data\nseq_len = 20 # choose sequence length\n\n","metadata":{"_uuid":"3333a7ef79ad756a4b0190e8aef2ed8b3624d7d8","_cell_guid":"e577ab98-344e-4a8e-88c2-09a7d45834b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to create train, validation, test data given stock data and sequence length\ndef load_data(stock, seq_len):\n    data_raw = stock.as_matrix() # convert to numpy array\n    data = []\n    \n    # create all possible sequences of length seq_len\n    for index in range(len(data_raw) - seq_len): \n        data.append(data_raw[index: index + seq_len])\n    \n    data = np.array(data);\n    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]));  \n    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));\n    train_set_size = data.shape[0] - (valid_set_size + test_set_size);\n    \n    x_train = data[:train_set_size,:,:].copy()\n    x_train[:,-1,1:]=[0]*(x_train.shape[2]-1)\n    y_train = data[:train_set_size,-1,1:4]\n    \n    x_valid = data[train_set_size:train_set_size+valid_set_size,:,:].copy()\n    x_valid[:,-1,1:]=[0]*(x_valid.shape[2]-1)\n    y_valid = data[train_set_size:train_set_size+valid_set_size,-1,1:4]\n    \n    x_test = data[train_set_size+valid_set_size:,:,:].copy()\n    x_test[:,-1,1:]=[0]*(x_test.shape[2]-1)\n    y_test = data[train_set_size+valid_set_size:,-1,1:4]\n    return [x_train, y_train, x_valid, y_valid, x_test, y_test]\n\ndef training_split_by_symbol(df_stock_norm_by_symbol):\n    x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df_stock_norm_by_symbol, seq_len)\n    return((x_train, y_train, x_valid, y_valid, x_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"symbol_list=[i for i in df_stock_norm.columns if (~i.find(\"symbol\"))]\nprint(symbol_list[0])\nx_train_o, y_train_o, x_valid_o, y_valid_o, x_test_o, y_test_o = load_data(df_stock_norm[df_stock_norm[symbol_list[0]]==1].sort_index(), seq_len)\nfor i in symbol_list[1:]:\n    print(i)\n    temp=training_split_by_symbol(df_stock_norm[df_stock_norm[i]==1].sort_index())\n    x_train_o=np.concatenate((x_train_o,temp[0]),axis=0)\n    y_train_o=np.concatenate((y_train_o,temp[1]),axis=0)\n    x_valid_o=np.concatenate((x_valid_o,temp[2]),axis=0)\n    y_valid_o=np.concatenate((y_valid_o,temp[3]),axis=0)\n    x_test_o=np.concatenate((x_test_o,temp[4]),axis=0)\n    y_test_o=np.concatenate((y_test_o,temp[5]),axis=0)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_o.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# row_num=5\n# print(\"Train Input First row Values\")\n# print(x_train_o[row_num])\n# print(\"Train Output First row Values - Normalized\")\n# print(y_train_o[row_num])\n# print(\"Train Output First row Values - Orignal\")\n# print(min_max_scaler.inverse_transform([np.concatenate([[0],y_train_o[row_num],[0]])]))\n# print(\"Train Output Dataframe - for checking the Logic\")\n# df[df.symbol=='ADSK'].iloc[row_num+(seq_len-1),:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model and validate data <a class=\"anchor\" id=\"4-bullet\"></a> \n- RNNs with basic, LSTM, GRU cells\n","metadata":{"_uuid":"a495f69f3a265f419cd5aa0a4119a8b0392365fd","_cell_guid":"4375362b-7a7c-4572-947c-91f46c4fd77c"}},{"cell_type":"code","source":"y_train_o.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Basic Cell RNN in tensorflow\n\nindex_in_epoch = 0;\nperm_array  = np.arange(x_train_o.shape[0])\nnp.random.shuffle(perm_array)\n\n# function to get the next batch\ndef get_next_batch(batch_size):\n    global index_in_epoch, x_train_o, perm_array   \n    start = index_in_epoch\n    index_in_epoch += batch_size\n    \n    if index_in_epoch > x_train_o.shape[0]:\n        np.random.shuffle(perm_array) # shuffle permutation array\n        start = 0 # start next epoch\n        index_in_epoch = batch_size\n        \n    end = index_in_epoch\n    return x_train_o[perm_array[start:end]], y_train_o[perm_array[start:end]]\n\n# parameters\nn_steps = seq_len\nn_inputs = x_train_o.shape[2]\nn_neurons = 50 \nn_outputs = y_train_o.shape[1]\nn_layers = 3\nlearning_rate = 0.001\nbatch_size = 25\nn_epochs = 50 \ntrain_set_size = x_train_o.shape[0]\ntest_set_size = x_test_o.shape[0]\n\ntf.reset_default_graph()\n\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.float32, [None, n_outputs])\n\n# use GRU cell\nlayers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)\n         for layer in range(n_layers)]\n                                                                     \nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n\nstacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) \nstacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\noutputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\noutputs = outputs[:,n_steps-1,:] # keep only last output of sequence\n                                              \nloss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error \noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) \ntraining_op = optimizer.minimize(loss)\n                                              \n# run graph\nwith tf.Session() as sess: \n    sess.run(tf.global_variables_initializer())\n    for iteration in range(int(n_epochs*train_set_size/batch_size)):\n        x_batch, y_batch = get_next_batch(batch_size) # fetch the next training batch \n        sess.run(training_op, feed_dict={X: x_batch, y: y_batch}) \n        if iteration % int(5*train_set_size/batch_size) == 0:\n            mse_train = loss.eval(feed_dict={X: x_train_o, y: y_train_o}) \n            mse_valid = loss.eval(feed_dict={X: x_valid_o, y: y_valid_o}) \n            print('%.2f epochs: MSE train/valid = %.6f/%.6f'%(\n                iteration*batch_size/train_set_size, mse_train, mse_valid))\n\n    y_train_pred = sess.run(outputs, feed_dict={X: x_train_o})\n    y_valid_pred = sess.run(outputs, feed_dict={X: x_valid_o})\n    y_test_pred = sess.run(outputs, feed_dict={X: x_test_o})\n    ","metadata":{"_uuid":"b5015613883c52563da5f8e5be8d6adeb274e420","_cell_guid":"dde96de3-fef3-45cf-959c-22171a28a3b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrn=random.randint(1, 100)\nprint(\"Orignal\")\nprint(min_max_scaler.inverse_transform([np.concatenate([[0],y_test_o[rn],[0]])])[0][1:-1])\nprint(\"Predicted\")\nprint(min_max_scaler.inverse_transform([np.concatenate([[0],y_test_pred[rn],[0]])])[0][1:-1])\n# y_test_o[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# y_test_pred[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\ntrain_data = TensorDataset(torch.from_numpy(x_train_o), torch.from_numpy(y_train_o))\nval_data = TensorDataset(torch.from_numpy(x_valid_o), torch.from_numpy(y_valid_o))\ntest_data = TensorDataset(torch.from_numpy(x_test_o), torch.from_numpy(y_test_o))\n\nbatch_size = 400\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nval_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Predictions <a class=\"anchor\" id=\"5-bullet\"></a> ","metadata":{"_uuid":"a88583a9effb6f1f84e13b8853adca62578c2992","_cell_guid":"9035de79-0f5c-4b6b-bc86-0d8d4e2f392e"}},{"cell_type":"code","source":"y_train_o.shape","metadata":{"_uuid":"2923055320991790fe0c8c0ffa3e75558e5c313a","_cell_guid":"412b33a3-1e02-4ed5-9846-9984022492bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ft = 0 # 0 = open, 1 = close, 2 = highest, 3 = lowest\n\n## show predictions\nplt.figure(figsize=(15, 5));\nplt.subplot(1,2,1);\n\nplt.plot(np.arange(y_train_o.shape[0]), y_train_o[:,ft], color='blue', label='train target')\n\nplt.plot(np.arange(y_train_o.shape[0], y_train_o.shape[0]+y_valid_o.shape[0]), y_valid_o[:,ft],\n         color='gray', label='valid target')\n\nplt.plot(np.arange(y_train_o.shape[0]+y_valid_o.shape[0],\n                   y_train_o.shape[0]+y_test_o.shape[0]+y_test_o.shape[0]),\n         y_test_o[:,ft], color='black', label='test target')\n\nplt.plot(np.arange(y_train_pred.shape[0]),y_train_pred[:,ft], color='red',\n         label='train prediction')\n\nplt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_valid_pred.shape[0]),\n         y_valid_pred[:,ft], color='orange', label='valid prediction')\n\nplt.plot(np.arange(y_train_pred.shape[0]+y_valid_pred.shape[0],\n                   y_train_pred.shape[0]+y_valid_pred.shape[0]+y_test_pred.shape[0]),\n         y_test_pred[:,ft], color='green', label='test prediction')\n\nplt.title('past and future stock prices')\nplt.xlabel('time [days]')\nplt.ylabel('normalized price')\nplt.legend(loc='best');\n\nplt.subplot(1,2,2);\n\nplt.plot(np.arange(y_train_o.shape[0], y_train_o.shape[0]+y_test_o.shape[0]),\n         y_test_o[:,ft], color='black', label='test target')\n\nplt.plot(np.arange(y_train_pred.shape[0], y_train_pred.shape[0]+y_test_pred.shape[0]),\n         y_test_pred[:,ft], color='green', label='test prediction')\n\nplt.title('future stock prices')\nplt.xlabel('time [days]')\nplt.ylabel('normalized price')\nplt.legend(loc='best');\n\ncorr_price_development_train = np.sum(np.equal(np.sign(y_train_o[:,1]-y_train_o[:,0]),\n            np.sign(y_train_pred[:,1]-y_train_pred[:,0])).astype(int)) / y_train_o.shape[0]\ncorr_price_development_valid = np.sum(np.equal(np.sign(y_valid_o[:,1]-y_valid_o[:,0]),\n            np.sign(y_valid_pred[:,1]-y_valid_pred[:,0])).astype(int)) / y_valid_o.shape[0]\ncorr_price_development_test = np.sum(np.equal(np.sign(y_test_o[:,1]-y_test_o[:,0]),\n            np.sign(y_test_pred[:,1]-y_test_pred[:,0])).astype(int)) / y_test_o.shape[0]\n\nprint('correct sign prediction for close - open price for train/valid/test: %.2f/%.2f/%.2f'%(\n    corr_price_development_train, corr_price_development_valid, corr_price_development_test))\n","metadata":{"_uuid":"6fa5eb60c8e8a5d3542547629c3e0df14e5c33a3","_cell_guid":"1bb898b9-943e-4e6f-b43d-8bdb6331cc4e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade tensorflow\nprint('Tensorflow version: {}'.format(tf.__version__))","metadata":{"_uuid":"2dc96fc968aa367482f1b7dc2018848af84bdb75","_cell_guid":"581be505-fdc8-4aec-a3a3-19cd28f4e324","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Time2VEC and model classes","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Time2Vector(Layer):\n      def __init__(self, seq_len, **kwargs):\n        super(Time2Vector, self).__init__()\n        self.seq_len = seq_len\n\n      def build(self, input_shape):\n        '''Initialize weights and biases with shape (batch, seq_len)'''\n        self.weights_linear = self.add_weight(name='weight_linear',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.bias_linear = self.add_weight(name='bias_linear',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.weights_periodic = self.add_weight(name='weight_periodic',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.bias_periodic = self.add_weight(name='bias_periodic',\n                                    shape=(int(self.seq_len),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n      def call(self, x):\n        '''Calculate linear and periodic time features'''\n        x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n\n        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n\n      def get_config(self): # Needed for saving and loading model with custom layer\n        config = super().get_config().copy()\n        config.update({'seq_len': self.seq_len})\n        return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SingleAttention(Layer):\n      def __init__(self, d_k, d_v):\n        super(SingleAttention, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n\n      def build(self, input_shape):\n        self.query = Dense(self.d_k, \n                           input_shape=input_shape, \n                           kernel_initializer='glorot_uniform', \n                           bias_initializer='glorot_uniform')\n\n        self.key = Dense(self.d_k, \n                         input_shape=input_shape, \n                         kernel_initializer='glorot_uniform', \n                         bias_initializer='glorot_uniform')\n\n        self.value = Dense(self.d_v, \n                           input_shape=input_shape, \n                           kernel_initializer='glorot_uniform', \n                           bias_initializer='glorot_uniform')\n\n      def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n        q = self.query(inputs[0])\n        k = self.key(inputs[1])\n\n        attn_weights = tf.matmul(q, k, transpose_b=True)\n        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        v = self.value(inputs[2])\n        attn_out = tf.matmul(attn_weights, v)\n        return attn_out    \n\n#############################################################################\n\nclass MultiAttention(Layer):\n  def __init__(self, d_k, d_v, n_heads):\n    super(MultiAttention, self).__init__()\n    self.d_k = d_k\n    self.d_v = d_v\n    self.n_heads = n_heads\n    self.attn_heads = list()\n\n  def build(self, input_shape):\n    for n in range(self.n_heads):\n      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n    \n    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n    self.linear = Dense(input_shape[0][-1], \n                        input_shape=input_shape, \n                        kernel_initializer='glorot_uniform', \n                        bias_initializer='glorot_uniform')\n\n  def call(self, inputs):\n    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n    concat_attn = tf.concat(attn, axis=-1)\n    multi_linear = self.linear(concat_attn)\n    return multi_linear   \n\n#############################################################################\n\nclass TransformerEncoder(Layer):\n      def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n        super(TransformerEncoder, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n        self.n_heads = n_heads\n        self.ff_dim = ff_dim\n        self.attn_heads = list()\n        self.dropout_rate = dropout\n\n      def build(self, input_shape):\n        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n        self.attn_dropout = Dropout(self.dropout_rate)\n        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n\n        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n        self.ff_dropout = Dropout(self.dropout_rate)\n        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n\n      def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n        attn_layer = self.attn_multi(inputs)\n        attn_layer = self.attn_dropout(attn_layer)\n        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n\n        ff_layer = self.ff_conv1D_1(attn_layer)\n        ff_layer = self.ff_conv1D_2(ff_layer)\n        ff_layer = self.ff_dropout(ff_layer)\n        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n        return ff_layer \n\n      def get_config(self): # Needed for saving and loading model with custom layer\n        config = super().get_config().copy()\n        config.update({'d_k': self.d_k,\n                       'd_v': self.d_v,\n                       'n_heads': self.n_heads,\n                       'ff_dim': self.ff_dim,\n                       'attn_heads': self.attn_heads,\n                       'dropout_rate': self.dropout_rate})\n        return config ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n      '''Initialize time and transformer layers'''\n      time_embedding = Time2Vector(seq_len)\n      attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n      attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n      attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n\n      '''Construct model'''\n      in_seq = Input(shape=(seq_len, 5))\n      x = time_embedding(in_seq)\n      x = Concatenate(axis=-1)([in_seq, x])\n      x = attn_layer1((x, x, x))\n      x = attn_layer2((x, x, x))\n      x = attn_layer3((x, x, x))\n      x = GlobalAveragePooling1D(data_format='channels_first')(x)\n      x = Dropout(0.1)(x)\n      x = Dense(64, activation='relu')(x)\n      x = Dropout(0.1)(x)\n      out = Dense(1, activation='linear')(x)\n\n      model = Model(inputs=in_seq, outputs=out)\n      model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n      return model\n\n\n    model = create_model()\n    model.summary()\n\n    callback = tf.keras.callbacks.ModelCheckpoint('Transformer+TimeEmbedding.hdf5', \n                                                  monitor='val_loss', \n                                                  save_best_only=True, verbose=1)\n\n    history = model.fit(X_train, y_train, \n                        batch_size=batch_size, \n                        epochs=35, \n                        callbacks=[callback],\n                        validation_data=(X_val, y_val))  \n\n    model = tf.keras.models.load_model('/content/Transformer+TimeEmbedding.hdf5',\n                                       custom_objects={'Time2Vector': Time2Vector, \n                                                       'SingleAttention': SingleAttention,\n                                                       'MultiAttention': MultiAttention,\n                                                       'TransformerEncoder': TransformerEncoder})\n\n\n###############################################################################\n'''Calculate predictions and metrics'''\n\n#Calculate predication for training, validation and test data\ntrain_pred = model.predict(X_train)\nval_pred = model.predict(X_val)\ntest_pred = model.predict(X_test)\n\n#Print evaluation metrics for all datasets\ntrain_eval = model.evaluate(X_train, y_train, verbose=0)\nval_eval = model.evaluate(X_val, y_val, verbose=0)\ntest_eval = model.evaluate(X_test, y_test, verbose=0)\nprint(' ')\nprint('Evaluation metrics')\nprint('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\nprint('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\nprint('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))\n\n###############################################################################\n'''Display results'''\n\nfig = plt.figure(figsize=(15,20))\nst = fig.suptitle(\"Transformer + TimeEmbedding Model\", fontsize=22)\nst.set_y(0.92)\n\n#Plot training data results\nax11 = fig.add_subplot(311)\nax11.plot(train_data[:, 3], label='IBM Closing Returns')\nax11.plot(np.arange(seq_len, train_pred.shape[0]+seq_len), train_pred, linewidth=3, label='Predicted IBM Closing Returns')\nax11.set_title(\"Training Data\", fontsize=18)\nax11.set_xlabel('Date')\nax11.set_ylabel('IBM Closing Returns')\nax11.legend(loc=\"best\", fontsize=12)\n\n#Plot validation data results\nax21 = fig.add_subplot(312)\nax21.plot(val_data[:, 3], label='IBM Closing Returns')\nax21.plot(np.arange(seq_len, val_pred.shape[0]+seq_len), val_pred, linewidth=3, label='Predicted IBM Closing Returns')\nax21.set_title(\"Validation Data\", fontsize=18)\nax21.set_xlabel('Date')\nax21.set_ylabel('IBM Closing Returns')\nax21.legend(loc=\"best\", fontsize=12)\n\n#Plot test data results\nax31 = fig.add_subplot(313)\nax31.plot(test_data[:, 3], label='IBM Closing Returns')\nax31.plot(np.arange(seq_len, test_pred.shape[0]+seq_len), test_pred, linewidth=3, label='Predicted IBM Closing Returns')\nax31.set_title(\"Test Data\", fontsize=18)\nax31.set_xlabel('Date')\nax31.set_ylabel('IBM Closing Returns')\nax31.legend(loc=\"best\", fontsize=12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not a finished product needs to impliment the Transformer learning and the Model optimization.","metadata":{}}]}